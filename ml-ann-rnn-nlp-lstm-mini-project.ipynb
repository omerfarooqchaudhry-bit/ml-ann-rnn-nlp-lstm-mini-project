{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409852aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "In this mission, you will use a text from Shakespeare (Shakespeareâ€™s first sonnet) to train a recurrent neural network (RNN) for language modelling.\n",
    "You can use any editor such as Jupyter Notebook or Spyder, to complete this mission in your computer.\n",
    "Write code that uses the Keras library to build, train, and test an RNN with the architecture described.\n",
    "Recommendations:\n",
    " Preprocessing: Tokenization, lowercasing, and removing punctuations might help, though maintaining the structure could be beneficial.\n",
    " Data Augmentation: Consider supplementing with other poetic or classical text sources.\n",
    " Model Choice: Use LSTMs or GRUs instead of simple RNNs to handle long-term dependencies.\n",
    " Regularization: Apply dropout to mitigate overfitting.\n",
    " Evaluation: Use perplexity or BLEU scores to assess generated text quality.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Read the text file as separate lines of text with open ('data.txt', 'r') as file:\n",
    "# Read the text\n",
    "# To read the text file of lines from Shakespeare plays, Use these lines of code: \n",
    "#Read the text file as separate lines of text\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open('data.txt', 'r') as file:\n",
    "        text = file.read()\n",
    "        lines = text.lower().split('\\n')\n",
    "    print(\"length of lines\", len(lines))\n",
    "    print(\"Text\", text[0:200])\n",
    "    print(\"Lines\", lines)\n",
    "    #Define words, vocabulary size and sequences of words as lines\n",
    "    from keras.src.legacy.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "    words = text_to_word_sequence(text)\n",
    "    print(\"words\", words)\n",
    "    print(\"these many wordds\", len(words))\n",
    "\n",
    "    import pandas as pd\n",
    "    word_series = pd.Series(words)\n",
    "\n",
    "    # Use value_counts() to get the count of each unique word\n",
    "    word_counts = word_series.value_counts()\n",
    "\n",
    "    # Create a DataFrame from the counts\n",
    "    word_df = pd.DataFrame({'Word': word_counts.index, 'Count': word_counts.values})\n",
    "\n",
    "    # See count of words in poem\n",
    "    print(\"words data frame\" ,word_df)\n",
    "\n",
    "    # create token for each word i.e assign it a spot in an array (work book 8-7)\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(words)\n",
    "    tokens = tokenizer.word_index\n",
    "    print(\"tokens\", tokens)\n",
    "    \n",
    "    \"\"\"\n",
    "    ''' Let's see how many words are in the \"library\" to train on.\n",
    "    The +1 is required, because later on when using to_categorical, \n",
    "    a preprocessing step on the target data, it creates size - 1 columns, \n",
    "    so we have to account for that somewhere, and we can do that here so there are enough target columns.\n",
    "    '''\n",
    "    \"\"\"\n",
    "    vocabulary_size = len(tokenizer.word_index) + 1\n",
    "    print(\"vocabulary size\", vocabulary_size)\n",
    "    sequences = tokenizer.texts_to_sequences(lines)\n",
    "    print (\"sequences\", sequences)\n",
    "\n",
    "    #Find subsequences \n",
    "    subsequences = []\n",
    "    for sequence in sequences:\n",
    "        for i in range(1, len(sequence)):\n",
    "            subsequence = sequence[:i+1]\n",
    "            subsequences.append(subsequence)\n",
    "        # print(\"subsequences\",subsequences)    \n",
    "\n",
    "    #visualize the new set-up\n",
    "    print(\"subsequences\", subsequences)\n",
    "\n",
    "    # Can use this embedding to call to display specific words in the dictionary at specific locations.\n",
    "    # In this example, 581 is outside the range of the embedding, so does not return a word.\n",
    "\n",
    "    _list=[26,189,581]\n",
    "    for k, v in tokens.items():    \n",
    "        if v in _list:\n",
    "            print(v,k)\n",
    "    \n",
    "    #  Find the location of specific words in the embedding:\n",
    "    _list=['trees','thou']\n",
    "    for k, v in tokens.items():    \n",
    "        if k in _list:\n",
    "            print(k,v)\n",
    "    \n",
    "    # Padding your sequences\n",
    "    # You need to have equal sequences for training. You will apply padding.\n",
    "    # Write these lines of code to implement the padding needed:\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    sequence_length = max([len(sequence) for sequence in sequences])\n",
    "    sequences = pad_sequences(subsequences, maxlen=sequence_length, padding='pre')\n",
    "    print(\"sequences\",sequences)\n",
    "    print(\"sequences shape\",sequences.shape)\n",
    "\n",
    "    #### 5. Split the lines of words into input and output.  \n",
    "\n",
    "    # The last word of each line is now the fartherst right word on each line because of all the padding zeros pushing it over the appropriate number of spots. You can grab the output (our 'label') now, because the next word in the line is always in the last column. No I can easily see what collection of words goes before any given word. Cool! \n",
    "    # x, y = sequences[:,:-1],sequences[:,-1]\n",
    "\n",
    "\n",
    "    # Encode the target labels\n",
    "    # Use these lines of code to encode your labels for training:\n",
    "    # \n",
    "\n",
    "    from keras.utils import to_categorical\n",
    "    x, y = sequences[:,:-1],sequences[:,-1]\n",
    "    print(\"x.shape,y.shape...\", x.shape,y.shape)\n",
    "    print(x[0:11])\n",
    "    print(y)\n",
    "    print(np.unique(y))\n",
    "    print(len(np.unique(y)))\n",
    "    \n",
    "    y = to_categorical(y, num_classes=vocabulary_size)\n",
    "\n",
    "    print(\"y shape...\", y.shape)\n",
    "    \n",
    "    print(\"y label...\",y)\n",
    "\n",
    "    \"\"\"\n",
    "    Define an RNN with the following layers:\n",
    "    An embedding layer with the following parameters:\n",
    "    The input dimension is vocabulary_size.\n",
    "    The output dimension is 100.\n",
    "    The input length is sequence_length - 1.\n",
    "    An LSTM layer with 100 units.\n",
    "    A dropout layer with a dropout rate of 10%.\n",
    "    A dense layer with the following parameters:\n",
    "    Activation function is softmax.\n",
    "    The number of units is vocabulary_size.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    from keras.models import Sequential \n",
    "    model = Sequential()\n",
    "\n",
    "    from keras.layers import Embedding\n",
    "    model.add(Embedding(input_dim = vocabulary_size, \n",
    "                    output_dim = 100,\n",
    "                    input_shape = (sequence_length - 1,)))\n",
    "\n",
    "# In other words, solve a 100 dimensional vector (output_dim) for each word in the vocabulary (input_dim) based on our length of 9 subsequences (input_shape)\n",
    "\n",
    "    from keras.layers import LSTM \n",
    "    model.add(LSTM(units = 100))\n",
    "\n",
    "    from keras.layers import Dropout, Dense \n",
    "    model.add(Dropout(rate=0.1))\n",
    "\n",
    "    #The output layer is a fully-connected layer the size of the vocabulary as it will present \n",
    "    # a probability for each word of it being the next word in the string.\n",
    "    model.add(Dense(units=vocabulary_size, activation ='softmax'))\n",
    "\n",
    "    print(\"model summary\",model.summary()) \n",
    "\n",
    "    \"\"\"\n",
    "    ## Compile the network\n",
    "    Build the network using:\n",
    "    - An adam optimizer\n",
    "    - The loss function is categorical_crossentropy\n",
    "    - The metric used is accuracy\n",
    "    \"\"\"  \n",
    "\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    from livelossplot import PlotLossesKeras\n",
    "\n",
    "    model.fit(x, y,\n",
    "          callbacks=[PlotLossesKeras()],\n",
    "          epochs = 500)\n",
    "\n",
    "    scores = model.evaluate(x, y, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "    _list=[np.argmax(model.predict(x[3:4]))]\n",
    "    for k, v in tokens.items():    \n",
    "        if v in _list:\n",
    "            print(\"The next word is: \",k) \n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
